{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to `pandas` — Exercises\n",
    "\n",
    "We continue exploring the Pandas package for simple data handling tasks using geoscience data examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Reading and writing files\n",
    "\n",
    "Pandas reads files from disk or the web — [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) is a list of all the 20 or so formats that it can read and write. A very common one is Excel's `.xlsx` format, so let's load one!\n",
    "\n",
    "Note that this requires `openpyxl` to be installed, eg with `uv add openpyxl` or `pip install openpxyl`.\n",
    "\n",
    "The data is the same as used in this study: http://www.kgs.ku.edu/PRS/publication/2003/ofr2003-30/index.html\n",
    "\n",
    "From that poster:\n",
    "\n",
    "> The Panoma Field (2.9 TCF gas) produces from Permian Council Grove Group marine carbonates and nonmarine silicilastics in the Hugoton embayment of the Anadarko Basin. It and the Hugoton Field, which has produced from the Chase Group since 1928, the top of which is 300 feet shallower have combined to produce 27 TCF gas, making it the largest gas producing area in North America. Both fields are stratigraphic traps with their updip west and northwest limits nearly coincident. Maximum recoveries in the Panoma are attained west of center of the field. Deeper production includes oil and gas from Pennsylvanian Lansing-Kansas City, Marmaton, and Morrow and the Mississippian.\n",
    "\n",
    "For Excel files, we can load specific sheets by passing the `sheet_name` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://github.com/scienxlab/datasets/raw/refs/heads/main/kgs/panoma_data.xlsx\"\n",
    "\n",
    "df = pd.read_excel(url, sheet_name='data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Without it, we get the first sheet, which in this case is not the data that we want, but it may still be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load all the worksheets as a dictionary by passing `sheet_name=None`, which gives us a dictionary of DataFrames, with the key being the sheet name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(url, sheet_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other formats are usually loaded in a similarly way, using the `pd.read_*` pattern: `pd.read_csv`, `pd.read_csv` and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Load the \"data\" sheet again, but use the \"columns\" sheet to set the types of the columns.\n",
    "\n",
    "You may need to think about:\n",
    "\n",
    "- Renaming one or more columns in the \"columns\" sheet.\n",
    "- Constructing a dictionary like `{'<column_name>: <type_as_string>'}`\n",
    "- Using that dictionary when loading the \"data\" sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Replacing `Facies`\n",
    "\n",
    "Facies uses an encoding, but there are more natural descriptions in the \"facies\" sheet.\n",
    "\n",
    "Make a new column called \"Lithology\" and populate it with the descriptions corresponding to the facies codes. Use an appropriate dtype.\n",
    "\n",
    "Remove the `-` from `\"Non-marine sandstone\" in the descriptions. \n",
    "\n",
    "Finally, drop the \"Facies\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Facies'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Locations\n",
    "\n",
    "Sometimes it's convenient to have 'group' data in the main DataFrame. For example, we could add columns from \"wells\" to `df`.\n",
    "\n",
    "Add the 3 columns. Make sure to use the appropriate types, eg timestamp for \"Completion date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "<div style=\"background: #e0ffe0; border: solid 2px #d0f0d0; border-radius:3px; padding: 1em; color: darkgreen\">\n",
    "\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "Using the `df` object, constructed from the sheet called \"data\", do the following:\n",
    "\n",
    "* Make a new dataframe with only the `Well Name`, `Depth` and `GR` columns.\n",
    "* How many rows are there in the `LUKE G U` well?\n",
    "* In the `CROSS H CATTLE` well, what is the mean of `GR` when depth > 850 m?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.loc[df['Depth'] > 850].groupby(df['Well Name'])\n",
    "grouped['GR'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data out\n",
    "\n",
    "Pandas can write to about 20 different formats. Writing data out is similarly simple to reading it in, using one of the range of `to_*` functions.\n",
    "\n",
    "In this case, we will go with a simple `csv` format, which [most people love](https://github.com/medialab/xan/blob/master/docs/LOVE_LETTER.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/Panoma_Field_Permian.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle, Feather, or Parquet files are more performant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('./data/Panoma_Field_Permian.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are comfortable with SQL, or have an existing database, we may wish to write our dataframe as a table there. We will use the Python implementation of [sqlite](https://www.sqlite.com/index.html), [sqlite3](https://docs.python.org/3/library/sqlite3.html). If you have an existing database you may prefer to look at [SQLalchemy](https://docs.sqlalchemy.org/) to create the connection instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('./data/panoma.db')\n",
    "df.to_sql('panoma_raw', con=connection, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use `read_sql` to get data from a SQL database instead of reading a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "info"
    ]
   },
   "source": [
    "<div style=\"background: #e0f0ff; border: solid 2px #d0e0f0; border-radius:3px; padding: 1em; color: navy\">\n",
    "\n",
    "<h3>High performance Pandas</h3>\n",
    "\n",
    "Note that for very large datasets, there are a few optional dependencies and settings you can use to speed up certain operations (big data, Boolean comparisons, lots of NaNs, etc).\n",
    "\n",
    "Read more in the docs, eg:\n",
    "\n",
    "- https://pandas.pydata.org/docs/user_guide/basics.html#accelerated-operations\n",
    "- https://pandas.pydata.org/docs/getting_started/install.html#performance-dependencies-recommended\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<p style=\"color:gray\">©2022 Agile Geoscience. Licensed CC-BY.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
